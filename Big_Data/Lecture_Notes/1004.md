#### distributed file systems

amazon s3: object size limit 5T

good practice: better code --> scale out --> scale up

large amount of huge files
- file is really big, > 5T, but not too much of them
- millions of PB files instead of billions of TB files(key-value model/object stroage for this case)
  - here we use file system/block storage
  - with huge amount of machines, one will fail
  - read model: random access or scan the file
  - update model: ranodm access or append
    - append: for sensors, logs, internediate data
      - and we desire atomic actions, parallel writing(not world wide system --> no too much writers)
  - priority: throughput over latency
- googlefs --> hadoop(opensourced)
  - hdfs, mapreduce, wide column store(hbase)
  
distributed file systems
- file systems: file hirearchy
- block storage: block/chunk/split/shard/partition --> same thing
  - block size 128mb: compromise latency and throughput
    - smaller block size --> too muchs blocks, too much latency
    - larger block size --> fewer blocks, lower parallelzation, lower throughput
- HDFS architecture
  - centralized architecture: coordinator/worker -- namenode/datanode
  - concurrently access the namemode, namenode(metadata only) -- datanode
    - duplicate of blocks, access the nearest one
  - namenode(process on machines): file namespace, file2block mapping, block locations
  - datanote(same as before): blocks as file in local FS, checksums, detect own failure
  - blockid: 64bit
  - client protocol(RPC): metadata operations --> datanode location(inlcuding replication locations, so multiple of datanode locations per block, sorted by distance) and block ids
    - client node often also one of the datanodes' machines
  - datanode protocol: connection to namenode(registration/heartbeat(then namenode can send back operations)/blockreport/blockreceived)
  - datatransfer protocol: data flow to/from just one datanode(check the another if the first fails), then datanode ship over to another one for replication
  - check some animations
  - reading: client as machine connect to all datanodes, checksum, report to namenode if case of failure
    - we can real sequentially or parallize
  - writing: namenode sends block id, datanodes for first/second... blocks, release locks, check minimal replication(1 by default), ack, then replicates async.ly
    - 64kbs of packets, aync wait for ACKs, close
  - number of replicas per file(3 by default)
    - rep1: samenode as the client, rep2: diff. rack, rep3, a node within the same rack as rep2, rep4: at most one rep per node, at most two rep per rack