### diemntionality reduction

auto-encoder
- encoder-decoder
    - F: R^n -> R^m, G: R^m -> R^n, assume m << n
    - ideallly GF x = x
- sample set S = {x_i iid~ v}
- true expetation: E_v f(x), empirical E_S f(x)
- loss function: R^n * R^n -> R
    - quadratic loss l(x1, x2) = .5 * |x1 - x2|_2^2
    - empirical risk, new data risk int l(x, GF x) dv(x)

linear auto encoder
- F(x) = Wx, G(z) = Vz
- objective: E .5 |x - VW x|_2^2
- for centered data and squared loss, optimal affine reconstruction is linear
- stacking linear layers does not help in terms of representation power
- is P unique?
- is pair of W, V unique -> no VAA^-1W
    - so not over-interpret the representation since it is easily rotated
- rank(P) <= min{n, m} = m