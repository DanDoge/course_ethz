### matrix approximation

alternating least squares
- ALS converges to a fixed point, may not be a global min
    - saddle point
- easy to augment by additional dimention

projection algorithms
- work with unconstrained representation, project to low rnak matrices
- A^0 = 0, A^t+1 = (A^t + eta Pi_Omega(A - A^t))_k
    - _k: best rank k approximation -> SVD
- convex relaxation of rank constraint: to make this a convex prob.
    - nuclear norm: sum sigma_i
    - |A|\_F = |sigma(A)|, |A|\_* = |sigma(A)|_1
    - convex envelope: largest convex function g s.t. g <= f
        - convex enveplope of rank(A) on |A|\_2 <= 1 is |A|\_*
        - then rank(A) < k relaxes to |A|_* < some value
    - shrink_tao(A) = argmin_B 1/2 |A - B|\_F^2 + tao |B|\_*
    = U diag(sigma_i - tao)_+ V^T
    - rank of shrink_tao(A) monotonically with tao
- A^0 = 0, A^t+1 = A^t + eta Pi_Omega(A - shrink_tao(A^t))
    - will converge to lim_t shrink_tao(A^t) 
    = argmin_B |B|_* + 1/2tao |B|_F^2, s.t. Pi_Omega(A - B) = 0
    - fits the data, and with a small norm
    - note low rank is not guaranteed, but sparse

randomised methods for SVD
- 