### dimentionality reduction

min_rank(p) < m R(P) = min_VW R(V, W)

projections
- im(P) is a linear subspace of dim. at most m
- given U in R^n, what is optimal lienar reconstruction map P s.t. R(P) is min. s.t. im(P) = U
- orthogonal projection: Pi_U(x) = argmin_x' |x - x'|^2
    - Pi_U(x) - x in orthogonal complementary of U
        - let Pi_U(x) - x = x + x' where x in U, x' in Or.Comp. U
    - Pi_U is linear: homogeneity, additivity
        - note that argmin f(y') = y0 + argmin f(y0 + y'), take out Pi_U(x)
    - for a given subspace U, the optimal projection matrix is representing the Pi_U(x)
    - e.g. projection to a line P = uu^t
    - got any linear subspace, P = UU^t, U = (u_1...k), Px = sum < u_i, x > u_i
        - < Px, x - Px > = < x, (P - P^2) x > = 0
            - self-adjointness of inner product and idempotency of orthogonal matrix
    - is autoencoder an ortho proj? in general no, with V = Wt yes, **does it requires V being an orthogonal matrix**
    - oblique projection matrix: P = VV+, where V+ = (VtV)^-1 Vt, in case V = (v_1...k) is a non-orthonormal basis
        - note PV = V, it is a projection, and rank constraint implies Pu = 0 for u in Ortho.Comp. of U
        - for any V, V+ = argmin_W R(W, V)
- which subspace U should we project to
    - min E |x - Px|^2 with P = UUt
    - E |x - Px|^2 - E |x|^2 = -E < x, Px >
    - for centered data: loss = 1/2 (Var x - Var Px)
    - equavalent to max Var Px = max tr(P E xx^t)
        - Var Px = E < Px, Px > = E < x, Px > = E tr(xt P x) = tr P E xxt
        - the optimal projection is full determined by cov matrix of data, (and expetation of data used for centering)
- PCA theorem
    - rank m projection optim. squared recons. loss on centered data is the proj. mat. by m ortho. principal eigenvectors of E xxt