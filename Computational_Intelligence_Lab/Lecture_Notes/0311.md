### PCA cont'd

which is the optimal projection
- consider diagonal case E xxt = diag(lambda_1...n)
    - prove P = sum_m eet is variance max. for fixed m
    - max tr(UUt diag) = sum_i lambda_i sum_j u_ij^2 s.t. UtU = I
    - sum_j u_ij^2 <= 1, othervise |Pe_i| > 1
    - UtU = I -> sum_ij u_ij^2 = m
    - so we have max lambda_i * U_i, with each U_i <= 1 and sum_i U_i = m
- spectral theorem: any symmetric and PSD matrix Sigma = Q diag Qt
- PCA: var. max projection for a cov matrix Sigma is P = UUt U = Q diag(1..., 0...)
    - max tr(UUt Sigma) = tr((QtU)t diag (QtU)), max QtU = I
    - linear AE performs a PCA, it is not guaranteed to identify the PCA basis, but only the principal space
        - review proof: center data, remove corr of data, do things on diag cases
- A symmetric with unit eigenvector u, Au = lambda u, lambda eigenvalue with largest abs
    - then |Au| = max_|v|=1 |Av|
- eigenvectors with distinct eigen values are orthogonal
    - proof: < Au, v > = < u, Av >
- iteratively get principal vector: A' = A - lambda uut

np.cov(X)
np.linalg.eigh(covar)

complexity: O(n^3)
cov for N observaitons in R^n O(Nn^2)

for m << n, we only need the first m eigen vectors
- power method for m = 1
    - init a random v ~ N(0, I), v_t+1 = Av_t / |Av_t|
    - lim v_t = u_1 if < v_0, u_1 > is not 0
    - represent in eigen basis then v_k ~ alpha_1 u_1 + sum alpha_i (lambda_i / lambda_1)^k u_i
- then iteratively get first m eigen vectors
- O(Tmn^2)

gradient descent
- Nabla_P l = (P - I)xxt
- then chain rule to W and V
- time complexity O(T(m + k)n^2), k being batch size, large k leads to small T

non-linear AE
- smaller noise, GD may not find the best retults