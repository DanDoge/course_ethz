### GD

convergence to optimal
- global: hard
- locally: consider quadratic approx.
    - consider convex quadratic objective, diagonalize to separable problem
    - shift the problem, take a gradient step
        - l(theta - eta grad) = (1 - lambda eta)^2 l(theta)
        - sufficient condition for decrease: eta < 2 / lambda_max
        - optical eta = 2 / (lambda_max + lambda_min)
            - weakest rate: (1 - lambda_min eta)^2 leq ((kappa - 1) / kappa)^2
    - smoothness: ensures gradient step works
- gradient norm: eps-critical point
    - with step size 1/L, at most 2L(Delta_0) / eps^2 steps, reach eps-critical point
    - smoothness is sufficient to fine eps-critical point with O(eps^-2) steps
- PL condition: gradient greater than 2 mu (l(theta) - l^*)
    - L-smooth and mu-PL -> convergence at (1 - mu/L)^k
    - in DNN PL hold possibly over a domain around local min
- saddle points
    - noisy GD may help
    - PL not help alsp
    - heavy ball: beta in .9 - .95
    - nesterov acc: estimate the next point, then make a gradient step there
    - adagrad: diff. lr for each dimension
    - adam: momentom over gradient and gradient norm
    - rmsprop