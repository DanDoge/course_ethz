### topic model

topic: distribution of words
- document: p(z | d): prob. of topic given document
- topics: p(w | z)
- two stage sampling
- LLH: sum_ij n_ij log p(w_j | d_i)
    - p(w_j | d_i) = sum_z p(w_j | z) p(z |d_i)
    - matrix multiplication here

LDA
- dirichlet distribuiton for prior of p(z)
    - p(v | alpha) propto Prod v_z^(alpha_z - 1.)
- P(X | U) = int prod p(x_t | U, v) p(v) dv
    - U: matrix of prob of word given a topic
    - then MLE for U
    - not for z since it is integrated out: multiple topics

prob. matrix decomposition
- U: word given topic, V: topic given document
- N^hat = UV: relative word freq., word given document, rank(N^hat) leq k
    - N_ij ~ s_i N^hat_ij
    - likelihood loss: sum N_ij log N^hat_ij, N^hat = UV, U, V geq 0, elementwise, and U, V sum to 1 per row/column
        - since we are working in LLH function, we cannot normalize N_ij
- revisiting non negative matrix facorization
    - l = .5 ||A - UV||_F^2, U, V geq 0

### narutal language model

word embeddings
- skip gram: llh of given word predict its neighbors
    - bilinear model: log p(v | w) = < z_v, z_w > + const
    - refined modeldiff embedding for condition and prediction and explicit bias
    - reformulate as a classification problem: negative samples from random word pairs
        - optimal bayes prob. p(true | v, w) = p(v, w | true) p(true) / sum
        - -> log odds of optimal bayes classifier are pointwise mutual information terms
        - balanced nagative sampling can be interpreted as max. mutual information of word co-occurrences
- GloVe: l = sum f(N_vw) | log N_vw - log N^hat_vw|^2
    - f: weighting function, ignore some non frequent word pair
    - ln N^hat(v, w) = < zeta_w, z_v >
    - lowrank matrix factorization in special cases: UV ~ log N
        - SGD