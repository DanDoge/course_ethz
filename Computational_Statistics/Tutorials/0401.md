### CV

```r
n = dim(Auto)[1]
### LOOCV estimate of error
for (i in 1:n) {
    fit.i <- lm(y ~ x, data = Auto[-i, ])
    y.hat[i] <-  predict(fit.i, newdata = data.frame(x = x[i]))
}
mean((y.hat - y)^2)

for (d in 1:5){
    for (i in 1:n) {
        fit.i <- lm(y ~ poly(x, d), data = Auto[-i, ])
        y.hat[i] <-  predict(fit.i, newdata = data.frame(x = x[i]))
    }
    loocv.error[d] <- mean((y.hat - y)^2)
}

# it does loocv automatically
glm.fit = glm(y ~ poly(x, d), data=Auto)
cv.glm(Auto, glm.fit)$delta[1]
```

```r
K <- 10
folds <- sample(cut(seq(1:n), breaks=K, labels=FALSE), replace=FALSE)
for (d in 1:5){
    for (i in 1:K){
        test.ind <- which(folds == i)
        # train on Auto[-test.ind]
        # predict on test.ind
    }
    error[d] <- mean(fold_error)
}

# no consistent results, random split in folds
# can seed by set.seed(0)
cv.glm(Auto, glm.fit, K=10)$delta[1]
```

```r
confint(fit)

boot.out = boot(data, coef, R = 1000)
boot.ci(boot.out, type=c("perc", others), index=1)

```

BS estimate tends to be less conservative
- stays more away from 0 than confint from linear model theory

