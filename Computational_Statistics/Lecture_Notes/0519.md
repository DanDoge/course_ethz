### adaptive kernel

assume each tree grows unitl each leave node has one sample
- prediction for new x in R^p: y^hat(x) = sum y_i^hat(x) / B
    - = 1/B sum_b sum_leaf_b beta_leaf^b 1_{x in R_leaf^b}
    = 1/B sum_b sum_leaf_B sum_i y_i 1_{y_i in R_leaf^b} 1_{x in R_leaf^b} 
    (beta is the mean in the cluster, in our case, only one elements)
    = sum_i (1/B sum_b sum_leaf_b 1{x in leaf} 1{y in leaf}) y_i
    = sum_i w_i y_i
    (w_i is p(x_i and x are in the same leafnode for a random tree), sum w_i = 1)

```r
rf = randomForest(X, Y, proximity=TRUE, importance=TRUE)
# proximity: i and j are in the same leaf node
round(100 * rf$proximity)
# 1 - proximity will be distance matrix D
MDSplot(rf, Y) # Y are labels, to color the points
MDSplot(rf, Y, k=5) # 5 dims, get a pairwise plot
ts = Rtsne(D, is_distance=TRUE)
plt(ts$Y, col=Y)

varImpPlot(rf) # acc/mse decrease/increase if var[i] is randomized

predict(rf) # out of bag prediction

partialPlot(rf, X, colnames(X)[1]) # change in responce if some predictor changes
# note for extreme values, the prediction will pleatou -> prediction is convex conbination of data
```

Bagging: bootstrap aggregate
- base precedure: function g^hat
- take bootstrap sample X, y, compute g^hat
    - repeat, get g^hat_1...b -> g^hat_bag = mean g^hat_i
- g^hat_bag = g^hat + (E g^hat* - g^hat) --> bootstrap estimate of bias of g^hat
    - we are adding the bias now -> so it is better to reduce the var.
    - Var y^hat(x) = Sigma^2 sum w_i^2
    - for bagged estimater: Var = sum_i (1/B sum_b w_i^b)^2
