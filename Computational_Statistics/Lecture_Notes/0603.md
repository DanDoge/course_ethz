### regressions

ridge
- argmin ||y - X beta||_2^2 + lambda ||beta||_2^2
- = argmin residual sum of squares s.t. beta_2 leq tau
- (X^T X + lambda I)^-1 X^T y
    - if X^T X diagonal -> analytical solutions
    - shrinkage along directions of small eigen directions
    - similar to PCA regression, keep k directions, but with soft cutoff
- X = UDV^T, X beta = XV V^T beta
    - let X' = XV, X^T X = diag(D^2)
lasso
- argmin ... + lambda ||beta||_1
- take the gradient: soft thresholding
- intercept not included practically

model selection
- argmin ... + lambda ||beta||_0

objectives
- convex? l2, l1
- rotationally invariant? (in terms of rotating x), l2
- sparse solutions? l1, l0

