### model selection

penalized estimator
- beta = argmin |y - X beta| + lambda |beta|_0
    - where beta_0 is counting non-zero entries
    - what AIC?
    - non-convex obj function
        - forward selection / backward selection
        - adding or removing variables: interms of R^2 value
        - with lambda is not zero, we will stop at some point where the improvement in MSE does not compensate for increase of lambda
        - for new test data we may stop even earlier

### density estimation

x_1...n ~iid from F
- F has a density funciton f, and f is smooth
- simple choice: histogram, small bins: low bias, high var.

dnorm(xvec, mean=0, sd=1)
cdf = cumsum()
sam = approxfun(cdf, xvec, rule=2, method="linear")
y = sam(runif(1000))

plot(density(y, bw))

kernel density estimator
- f(x) = 1/2nh # of samples in (x-h, x+h) interval
 = 1/n sum 1/h K(x - x_i / h), with K(delta) = 1/2 |delta| <= 1
- not cont not diff. 
    - Gaussian kernel or Epanechnikov 3/4 (1 - |x|)^2 1_{|x| <= 1}
- kernel: non-negative, symmetric, normalized
    - choise of bandwidth
    - MSE = E (f^hat(x) - f(x))^2
    = E (f^hat(x) - E f^hat(x))^2 + E (E f^hat(x) - f(x))^2    Var + bias^2
    - asymptotically for smooth f var ~ 1/nh + higher order terms, bandwidth should be sub(1/n)
        - bias ~ h^4 + higher order things
        - MSE = const h^4 + const / nh
        - we need h -> 0 and nh -> infty: h = n^-.2 MSE -> n^-.8
    - for higher dimensions var ~ 1/nh^d -> MSE will be n^-4/(4+d)
- bias = E K(x - x_i / h)/h - f(x)
= int K(x - x_i / h)/h f(z) dz - f(x) # taylor expand, please refer to lecture notes, note a change of var happens here
= something w.r.t. f''

- for var, consider the point in the sliding window, which is approx. a Ber var