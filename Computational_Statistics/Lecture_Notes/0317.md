# non parametric regression

more general model
- y = m(x) + eps, eps ~iid F, any distribution with mean 0
    - m generally assume smooth, m(x) = E y | x
- do kernel regression estimator
- m(x) = E y | x = int_y f(y | x) dy
= int_y f(y, x dy) / f(x)
- f(x) can be estimated
- f(x, y) = sum K(x - x_i / h) K(y - y_i / h) / nh^2
    - plug in int_y f(y, x dy) / f(x)
    = sum K(x - x_i / h) y_i / sum K(x - x_i / h)
    = sum w_i y_i / sum w_i
    - a weighted sum of y_i
- m(x) as a locally weightrf least squares estimator
    - m(x) = argmin  sum K(x - x_i / h) (y_i - m)^2

which(!is.na(data))
ks = ksmooth(x, y, kernel="box" or "normal", bandwidth=1)
line(ks$x, ks$y)

or do it manually

weights = exp(- (xvec\[k\] - temp)^2 / (2 * sigma^2))
weights = weights / sum(weights)
mhar\[k\] = sum(weights * y)

inference
- m(x_i) is linear function of observations: y^hat = S y
- S_ij = K(x_i - x_j / h) / sum_j K(x_i - x_j / h)
- Cov(y^hat) = Cov(Sy) = Cov(S eps) = E (S eps eps^t S^t) = sigma^2 SS^t
    - note that y = m(x) + eps
- tr(S) is used to measure flexibility/ DoF of model is a function of h
    - tr(S) goes up as h goes down(h to 0, tr(S) to n)
- residual of obs. i is too small by a factor 1 - S_ii
    - y^hat_i - E y_i = (1 - S_ii) eps_i + sum_j!=i S_ij eps_j
    - y_i - E y_i = eps_i

local polynomial non-parametric regression
- kernel regression m(x) = argmin sum K(x - x_i / h) (y_i - beta)^2
- generalize to beta(x) = argmin sum K(x - x_i / h) (y_i - beta_1 - ... - beta_p (x - x_i)^(p-1))^2
    - m(x) = beta_1(x), since the function is centered at x, only intercept needed
    - can reduce bias
    - for points near the boundary, sampled on the left and right are not balanced

weights = exp(- (xvec\[k\] - temp)^2 / (2 * sigma^2))
weights = weights / sum(weights)
X = cbind(rep(1, length(x)), x)
betahat = solve(t(X) %\*% diag(weights) %\*% X, t(X) %\*% (weights * y))
mhar\[k\] = betahat\[1\] + betahat\[2\] * xvec\[k\]