### regression

given y_i, x_i1, ..., x_ip, for i in 1, ..., n
- find f(x_i1, ..., x_ip) s.t. E\[y_i\] = f(x_i1, ..., x_ip)
- multiple linear regression: y_i = sum beta_j x_ij + eps_i
    - assume eps_i are iid, E\[eps_i\] = 0, Var\[eps_i\] = sigma^2
    - matrix notation: x^(j) = (x_ij), j-th predictor, x_i = (x_ij), predictors for the i-th observation, beta, unknown parameter vector
    - y_i = x_i^T beta + eps_i, Y = X beta + eps
        - where X = (x^(j)), n * p matrix
    - often x_i1 = 1 to model the interecept
    - rank(X) = p only possible when p <= n, we assume this always holds
    - still considered linear regression for y = beta_1 + beta_2 log(x) + beta_3 x^2 + eps
        - linear in beta
    - goal
        - good fit -> small error term
        - good parameter estimater -> understand effect of diff. predictors
        - good prediction -> get expect value y_n+1 given x_n+1
        - uncertainty + significance for the above -> confidance interval + hypothesis test
        - good model -> update model by add or drop predictor variable
- least square estimate
    - estimate beta hat = argmin |Y - X beta|^2
    - beta hat = (XtX)^-1 Xt Y unique solution if rank(X) = p
    - residuals r_i = y_i - x_i^T beta_hat, sigma_hat^2 = 1 / (n - p) sum r_i^2
        - to be unbiased
- assumptions
    - E y_i = x_i t beta -> E eps_i = 0
    - measurement of x_i are exact
    - homoskedacity, Var eps_i === sigma^2
    - Cov(eps_i, eps_j) == 0, for all i != j
    - eps_i are jointly normal -> y_i are also jointly normal
- geometrical interpretation, project Y to p-dimentional space spanned by x^(j) (there are p of them)
    - X beta min |Y - X beta|_2^2
    - orthogonal projection onto X
    - r = Y - Y^ = Y - X beta must be orthogonal rT x^(j) = 0, X^T r = 0
    - X beta = X (XtX)^-1 Xt Y = PY
        - Pt = P, P^2 = P, tr(P) = tr(I) = p -> P is a projection
        - r = Y - X beta = (I - P) Y, orthogonal projection onto the orthogonal complement of X