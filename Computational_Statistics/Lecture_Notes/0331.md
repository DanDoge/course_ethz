### bootstrap

sample x_1...n ~ unknow f
- estimate theta^hat = g(x_1...n)
- want Var theta^hat
    - generate bootstrap sample x*_1...n ~ F_n = sum delta(x_1)
        - which is resample with replacement from the original estimation
    - compute theta*^hat from bootstrap samples
    - repeat, and estimate Var theta^hat as Var* theta*^hat

```r
n = 100
X = rnorm(n)
# we want the mean
# sd(X) / sqrt(n) is estimated std of mean(X)
# mean(X) / (std(X) / sqrt(n)) ~ t(n-1), definition of t dist.
mean(X) + c(-1, 1) * qt(0.975, df=n-1) * sqrt(var(X) / n)

B = 100
mub = numeric(B)
for (boot in 1:B){
    bsam = sample(1:n, replace=TRUE)
    xbsam = X[bsam]
    mub[boot] = mean(xbsam) # replace with median get a est of the median
}
varb = var(mub)
mean(X) + c(-1, 1) * qt(0.975, df=n-1) * sqrt(varb)
```

```r
require(glasso)
lambda = 0.9 # less lambda, denser graph
gl = glasso(cov(X), lambda, nobs=nrow(X))
image(gl$wi)
heatmap(1 * (gl$wi != 0), symm=TRUE)
```

realworld: f -> x_1...n -> theta^hat(x_1...n)
BS world : f^hat -> x\*_1...n -> theta\*^hat(x_1...n)
- idea: theta^hat - theta ~(in probability) theta*^hat - theta^hat
    - since in the real world theta^hat might be biased, so in BS world the same estimation formula is also biased
- if true Var*(theta*^hat) / Var theta^hat -> 1
    - (E theta*hat - theta^hat) / E theta^hat - theta -> 1, in the biased world
    - generally holds if x_1..n are iid and limiting distribution of theta^hat_n is normal
- under general reg. conditions 
    - sqrt(n) (theta^hat_MLE - theta) ~ N(0, I^-1(theta))
    - sqrt(n) (theta*^hat_MLE - theta^hat) ~ N(0, I^-1(theta))
- confidence interval (for biased distribution?)
    - symm CI can fail if it is skewed
    - theta^hat - theta <= q(theta*^hat - theta^hat, 1 - alpha/2), if dist. are the same, w.p. 1 - alpha
    - CI = [ 2theta^hat +- q*(theta*^hat, 1 +- alpha/2) ]
        - is not q*(theta^hat, 1 +- alpha/2)

```r
quantile(mub, 0.975)
2 * median(X) - quantile(mub, c(0.975, 0.025))
```

parametric bootstrap
- f^hat is not f_n but f^hat(alpha^hat)
- better approximation esp. for small n, if model class is approparate
- more bias if not

BS for regression
- paired BS does not capture Var(y | X = x)
- residual BS, eps_i = y_i - m^hat(x_i)
    - keep m^hat fixed, draw BS samples of residuals -> get new data for y*_i
    - then we have estimate of Var

double BS
- BS and BS, so true value is in the CI