### classification

given data (x_i, y_i) ~ iid
- y in {0...J-1}, goal estimate pi_j(x) = p(y = j | x)
- Bayes classifier
    - mis classification error p(C(x_new) != y_new)
    - C_bayes(x) = argmax_j p(y = j | x) so it min. mis classification error
    - bayes for some mixture: prob of correct is (linear in) total variation distance bet. two densities p(x | y = j)
        - TV = 1 -> prob = 1
        - TV = 0 -> prob = 1/2

LDA: classifier c^hat(x) = argmax Pi^hat_j(x)
- model: p(y = j) = p_j with sum p_j = 1, and p(x |y = j) = N(mu_j, Sigma)
    - Sigma the same for all j
- p(y = j | x) = Bayes
- estimate p_j, mu_j, Sigma
    - and c^hat(x) = argmax_j log(p^hat_j) -1/2 (x - mu^hat_j)^T Sigma^hat^-1 (x - mu^hat)
    - for the decision boundary, quadratic terms cancels out

```r
# LDA
# for class with the same samples, drop log(p_j)
mu = as.matrix(arrregate(X, by = list(y = Y), FUN = mean))
# mu[, 1] are label of y
xres = X - mu[Y, -1] # x - mu
covhat = cov(xres) # somehow biased
# for QDA covhat = covhati = list()
# selection = train[which(Y[train] == j)]
# covhat[[j]] = cov(xres[selection, ])
# and also include det(2 pi Sigma)^-.5 into the delta
covhati = solve(covhat)
phat = table(Y) / n

n = nrow(x)
delta = matrix(nrow = n, ncol = J)
for (j in 1:J){
    dist_to_mean = sweep(x, 2, mu[j, -1], FUN="-") # dist to mu_j, n * p
    delta[, j] = log(phat[j]) * diag(-0.5 * dist_to_mean %*% t(covhati) %*% t(dist_to_mean))
}

yhat = apply(delta, 1, which.max)

mean (yhat != y)
table(Y, yhat) # confusion matrix
```