### bagging, boosting

boosting
- functional gradient descent
- for regression
    - fit g^hat_1 to data, set f^hat_1 = gamma g^hat_1
    - loop over to fit residuals, and adding the shrinked version to f^hat_M

```r
X = matrix(rnorm(n * q), nrow=n) %*% matrix(rnorm(q * p), ncol=p)
predboost = rep(0, n)

for (m in 1:M) {
    residual = Y - predboost
    tree = randomForest(X, residual, maxdepth=2, ntree=1, sampsize=round(n), replace=FALSE) # no bagging here, each boosting round trains on full training data

    predboost = predboost + shrinkage * predict(tree, X)
}

```

general case
- loss function loss(f)
    - e.g. for classification loss(f) = sum log(1 + exp(-y_i f(c_i)))
        - leads to logitboost
    - then it is the same as weight samples that we are still wrong
    - i.e. AdaBoost if loss is exp(-y_i f(x_i))
    - MSE -> L2 boost