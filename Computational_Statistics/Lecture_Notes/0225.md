### linear regression

x, beta are fixed, eps are random, and thus y are random
- and thus beta^hat is also random
- Cov(Y^) = Cov(PY) = sigma^2 P^2 = sigma^2 P
- Cov(r) = sigma^2 (I - P) -> residuals are correlated while random error is not
    - E sum r_i^2 = sum E r_i^2 = sum Var(r_i) = sigma^2 sum 1 - P_ii = sigma^2 (n - p)
- assume Gaussian error, eps_i ~ iid N(0, sigma^2)
    - beta^ = beta + (XtX)^-1Xt eps
        - do a hypothesis test on this
    - Y^ = Xbeta + P eps
    - r = (I - P) eps, note that P = X(XtX)^-1Xt, (I - P) is the projection to complement space of X, thus (I - P)Xbeta = 0
    - sigma^hat^2 ~ sigma^2 / (n - p) chi^2_n-p
- tests and confidence regions
    - eps ~ iid N(0, sigma^2), n large enough s.t. CLT works
    - H_o beta_j = 0, H_a beta_j is not 0
    - beta_j ~ N(beta_j, sigma^2(XtX)^-1_jj)
    - under null hypothesis -> beta_j ~ N(0, sigma^2(XtX)^-1_jj)
        - estimate sigma^2
        - beta_j / sqrt(sigma^2(XtX)^-1_jj) ~ N(0, 1)
        - T_j = beta_j / sqrt(sigma^hat^2(XtX)^-1_jj) ~ t_n-p
            - significance of effect of x^(j) after substract effect of all other predictors
            - may be problematic: both x1 and x2 are correlated and correlated with y
                - both xi will have small significance w.rt. prediction y after cancelling out another
                - might test effect group of variables

for x^(i) and x^(j) orthogonal, the prediction beta_i and beta_j does not change if either one is dropped