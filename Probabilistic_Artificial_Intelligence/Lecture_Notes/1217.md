### model based Deep RL

why model based
- for planning
- reduce sample complexity
- start with policy and data
- iterate: roll out policy, learn a model for f, r from data, plan a new policy

planning with a known deterministic model
- assume have a function x = f(x, a)
- objective: max sum gamma^t r(x_t, a_t), where x_t+1 = f(x_t, a_t)
- explicitly optimize over infinite horizom is not possible
    - optim. fixed H horizons
    - observe x_t, optimize over next H steps, take a_t
- analytically, gradient is possible
    - but note the loca lminima and vanish/exploding gradients
    - random shooting methods: sample m set of random action sequences
    - pick the seq that max. J_H(a_t:t+H-1)
- using value estimate: J_H = sum gamma r(x, a) + gamma^H V(x_t+H)
    - e.g. for H=1 is just the greedy policy w.r.t. V
- for stochastic transition models
    - optimize expected performance
    - how to integrate? sample trajectory with fixed action sequences
    - suppose transition model is reparametrizable x = f(x, a, eps)
        - then sample M eps then J_H = 1/M sum_i sum_t ...
- using parametrized policies
    - J = E_x~mu sum_tao gamma^tao r_tao + gamma^H Q(x, pi(x, theta))
    - with H = 0, this is identical to DDPG

unknown dynamics: unknown f and r
- iter: roll out pi, learn f, r, plan a policy pi
- due to Markovian of MDP: transitions andrewards are conditionally independent
    - regression problem
    - asuume conditional Gaussians x_t+1 ~ N(mu(x_t, a_t; theta), Sigma(x_t, a_t; theta))
- MAP estimate is not enough -> error is exploited during planning
    - uncertainty must be captured
    - dependent behavior p(f | D) and independent randomness p(x | f, x, a)
    - sample m MDP/f, and in each MDP, sample trajectory