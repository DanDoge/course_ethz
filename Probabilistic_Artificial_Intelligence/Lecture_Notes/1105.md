### bayesian deep learning

weight decay
- equivalent to MAP inference in BNN
- theta = argmin -log p(theta) - sum log p(y | x, theta)
    - SGD
- variational inference
- MCVI
    - separate aleatoric and epistemic uncertainty by Var E y and E Var y
- dropout as VI
    - thus also should drop neurons during prediction