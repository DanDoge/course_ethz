### markov decision processes

MDP
- world in n states, and agent has set of m actions
    - transition prob p(x' | x, a)
    - agent has a reward function r(x, a)
    - assume P and R are known ahead of time
- policy: mapping from states to actions/prob dist of policies
    - induces MC: p(x_t+1 | x_t) = sum of pi(a | x_t) p(x' | x, a)
    - expected value J(pi) = E sum of gamma^i r(x_i, pi(x_i))
        - gamma: discount factor in [0, 1)
    - value function V^(pi)(x) : expected alue conditioned on starting point x
        - V(x) = r(x, pi(x)) + gamma sum of p(x' | x, pi(x)) V(x')
        - V = r + gamma T V, if gamma < 1, V = (1 - gamma T)^-1 r
            - cubic in num of states
        - fix point iteration: initialize V_0 as 0, iterate the above forluma
            - for all two guesses, the distance shrinks after one iteration
            - for sparse transitions -> computational adv.
    - greedy policy w.r.t. V
        - pi_g^v(x) = argmax_a r(x, a) + gamma sum p(x' | x, a) V(x')
        - each value function induces a policy, and each policy induces a value function
        - optimal policy is a fixpoint of above iteration
    - policy iteration: random policy, compute V, compute greddy pi
        - monotonically improve V_t+1(x) >= V_t(x)
        - converge to optimal policy in poly time
            - but requires computation of value function in each iteration
    - value iteration: for optimal policy, it holds that
        - V(x) = max r(x, a) + gamma sum p(x' | x, a) V(x') = max_a Q(x, a)
        - convergence proof: bellman update is also a contraction
            - |BV - BV'|_infty = max_x |BV(x) - BV'(x)|
            = max_x |max_a Q(x, a) - max_a Q'(x, a)| # diff of max is less than max of diff
            <= max_x max_a |Q(x, a) - Q'(x, a)|
            = max_x max_a |r(x, a) + gamma sum p(x' | x, a) V(x') - r(x, a) - gamma sum p(x' | x, a) V'(x')|
            = gamma max_x, a |sum p(x' | x, a) (V(x') - V'(x')) |
            <= gamma |V - V'|
        - contraction concludes a single fix points
        - eps-optimal solution in polynomial iterations
- POMDP
    - states being a distribution over n states
        - and since the exact states are not known, future states and observations are not independent
    - transition model b_t+1(x) ~ likelihood * prior = p(y | x)p(x)
    = p(y | x) sum b_t(x') p(x' | x)
        - note the transition model is random only w.r.t. observation y
        - so no fixed transition matrix available?
    - policy tree: each node prepresents a action to be taken
        - and then given observations, branch the node to |O| children nodes, then take more actions, and repeat
        - for each policy tree, define an alpha vector denote the return given initial state p  
            - then expected value given belief state is \< b, alpha \>
            - optimal policy is then take the maximum of these hyperplanes, which is for each belief state, act as defined in one policy tree
            - see https://www.cse.iitd.ac.in/~rohanpaul/teaching/data/2021-COL864/slides/L13-POMDPs.pdf