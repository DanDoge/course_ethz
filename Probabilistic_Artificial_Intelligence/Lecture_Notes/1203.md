### reinforcement learning

TD learning of value function
- on policy, not optimal
- for off policy cases, update state-action functions
    - q(x, a) = (1 - alpha)q(x, a) + alpha(r + gamma q(x', pi(x')))
- TD learning as SGD
    - L(v(x); x, r) = 1/2 (v(x) - r - gamma E_x' v(x'))^2
    - partial L = (v(x) - r - gamma E_x' v(x')), note E_x' v(x') is constant
        - assume x' ~ p(x' | x, a), v(x) - r - gamma v(x'), is unbiased estimator of the derivative, TD error
    - v(x) = v(x) - alpha partial L
- supppose q(x, a; theta) = theta^t phi(x, a)
    - L = 1/2 (q(x, a; theta) - r - gamma max q(x, a; theta))^2
    - partial L = (q(x, a; theta) - r - gamma max q(x, a; theta)) * partial q(x, a; theta)
        - unstable 'labels', changeing per gradient step
        - update per 'epoch'
        - maximization bias: overfitting to some previous max action
    - softmax exploration p(a | x) = softmax(q(x, a) / T)
    - double DQN: L = 1/2 (q(x, a; theta) - r - gamma q(x, a*(theta); theta_old))^2
        - take gradient w.r.t. argmax? also does not scale with |A|
    - learn a parameterized policy pi(x ; theta)
        - max J(theta) = 1/m sum_z (sum_i gamma^i r_z^i = tau_z)
        - partial J(theta) = E_tau~pi(theta) r(tau) partial log pi(theta)(tau)
        - where pi(theta)(tau) = p(x_0) prod pi(a_t | x_t; theta) p(x_t+1 | x_t, a_t)
        - variance reduction by baseline
            - E [(r(tau) - b) partial log pi(theta)(tau)] = E [r(tau) partial log pi(theta)(tau)] - b E [partial log pi(theta)(tau)], note E score function is 0
            - furthermore E [sum (r(tau) - b(tau_0...t-1)) partial log pi(a_t | x_t; theta)]
                - b(tau_0...t-1) = sum gamma^t' r_t'