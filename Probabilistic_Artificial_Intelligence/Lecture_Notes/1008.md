### Gaussian process

ridge regression: only use one estimation of the weights
- lost a lot of variance in weights -> as seen in the sigma in predcition
  - i.e. the xSigmax term(epistemic term)
- while BLR averaging all w acc. to posterior

motivation to GP
- what about nonlinear functions
  - linear models on polynomials -> factorial terms need to consider
  - kernel trick: (1 + x1 x2)^n
    - reformulate that only inner products matter -> apply to BLR?

weight v.s. function space view
- only predictions are considered, weights are only an intermediate
  - suppose in discrete space, we estimate f = \[f_i\], f = X w -> f ~ N(0, XX^T)
  - inner product get! and all w disappear, y ~ N(0, XX^T + sigma I)
  - suppose a new x arrives, we condition the y_n on all other ys
    - y_n ~ N(mu(x_n, y_1...n-1), simga(x_n)), note that posterior sigma has nothing to do with y, the expected observation
- distribution of functions
  - where the inner product came into place
  - kernelize f ~ N(0, K), K = Phi Phi^T
- also works in infinite domains
  - the resulting random function: GP
  - predictive uncertainty and tractable inference
  - note that all finite marginals are multivariate Gaussians

GP
- an (usually) infinite set of random variables, indexed by X
  - iff. all finite marginals are Gaussian
- exists functions mu: X -> R and k X * X -> R
  - s.t. for A in X, F_A ~ N(mu_A, K_AA)
- GP marginals
  - p(f_x) = N(f(x); mu(x), k(x, x))
  - k must be symmetric, and positive definite (for all index set A)
- kernel function encodes assumptions about the correlation
  - k(x1, x2) = x1^T x2 -> degenerate to BLR
  - k(x1, x2) = Phi Phi^T [0, x, x^2] or sin(x)
  - squared exponential/RBF/GAussian kernel
    - smaill bandwidth -> dependence drop more when distance increases
  - exponential/Laplacian kernel
  - cov function determins smoothness of sample paths
    - SE kernel: analytic(infinitely diff)
    - exponential kernel: nowhere diff
    - MatÃ©rn kernel: v times diff
  - composition rules
    - k1 + k2 -> kernel of function f1 + f2
  - forms: stationary(k(x1, x2) = k\`(x1 - x2)), isotropic(k(x1, x2) = k\`(|x1 - x2|_p))
- prediction
  - p(f | X, Y) = GP(f; mu, k), note we are characterising f, not observation
  - convention: prior with zero mean, shift everything with mu(x)
  - sample from GP: sample some points and sample response on these finite points
    - sample s ~ N(0, I), cholosky deconposition of K = LL^T, return Ls