### gaussian process

optimizing kernel parameters
- k(x1, x2) = sigma_p^2 exp(- dist(x1, x2) / 2h^2)
  - sigma_p is output scale, h is length scale
- also p(y | f, x) = N(., sigma_n^2)
  - sigma_n is noise variance
- cross validation on predictive performance
  - f = argmax p(y_train | f, theta), then score theta acc. p(y_val | f, theta), choose the theta with most score
- or from the bayesian perspective: max. maginal likelihood of the data(the f is marginalized out)
  - max_theta p(y | x, theta) = int p(y | f, theta) p(f | theta) df
    - in case of underfitting: likelihood small(for all f in prior), prior large(less possible functions to put prob. mass on)
    - overfitting: likelihood large(for few f in prior class, small for all other f in the prior), prior small(a lot of functions to contain)
    - ideal: moderate and moderate
  - max_theta p(y | x, theta) = int p(y | f, theta) p(f | theta) df
    - the former is a N(f(X), simga_n^2) and the latter is the prior N(0, K(X, X)), note this is sum of two gaussians N(0, s^2) + N(0, K_f)
    - -> N(y | 0, K_y(theta)), where K_y(theta) = K_f(theta) + sigma_n^2 I
    - -> argmin. -log() = -1/2 y^T K_y^-1 y - 1/2 log |K_y|
      - first term is the alighment of the kernel with the actual data, second is volume of predictions considered
  - can using gradient descent, local optima exists
- empirical bayes method
  - estimating a prior distribution from data
  - integrating (rather than optimizing) over functions -> aginst overfitting
  - ...or place hyperprior on parameters
  - validation data is not eliminated

computational issues
- invert n*n matrices
- parallelism: e.g. GPyTorch, cubic scaling not addressed
- local GP: condition only on nearby points
  - consider all points near the query point if the kernel value is above some threshold
  - or, cluster input points to clusters and predict using cluster "center"
- approx kernel function, BLR on m dim. feature map
  - fourier transform a shift invariant kernel
  - a shift invariant kernel is PD iff p(w) is nonegative
    - scale the data s.t. int p(w) = 1
  - z_w = sqrt(2) cos(wx + b), b ~ [0, 2pi]
    - s.t. E(z(x), z_(y)) = k(x - y), triangular equality
  - kernel as an expectation -> sample
    - to lower variance
  - approx the kernel a.e. -> might be wasteful
- incuding points
  - if we have a set of representative points u, then p(f, f*| u) = q(f|u) * q(f*|u)
    - and then differenece of sparse GP methods only differs in the way to approximate q(f | u), idealy, it should be a delta function(u has enough information to reconstruct f)
  - subset of data
    - namingly, only predict the output base on some subset of data
  - subset of regressor
    - give up, read before final then