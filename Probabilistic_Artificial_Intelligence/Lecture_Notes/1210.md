### actor critic

advantage function
- A^pi(x, a) = Q^pi(x, a) - V^pi(x) = Q^pi(x, a) - E Q^pi(x, a)
    - max_A A(x, a) >= 0
    - optimal policy iff forall x, a A(x, a) <= 0
    - greddy policy pi(x) = argmax_a Q(x, a) = argmax_a A(x, a)
- d J(theta) = lim_T E_tao sum_t gamma^t G_t d log pi
             = sum_t E_tao gamma^t G_t d log pi
             = sum_t E_tao_t gamma^t G_t d log pi # tao_t: suffix of tao from step t
             = sum_t E_(x_t,a_t) gamma^t d log pi * E[G_t | x_t, a_t]
             = E_tao sum_t gamma^t Q(x_t, a_t) d log pi
    - note that Q(x_t, a_t) is (sort of) expectation of gamma^t -> variance reduction
- policy gradient theorem d J(theta) = E_(x,a)~pi Q(x, a) d log pi(a | x ; theta)
    - approx Q(x, a): critic, allows for online learning (note the bias)
- greedy policy pi(x) = argmax_a Q(x, a)
    - equivalent to theta = argmax E_x Q(x, pi(x ; theta_pi); theta_q)
    - gradients w.r.t. theta_pi: E_x d Q(x, pi(x; theta_pi); theta_q)
    = E_x d Q(x, a) d pi(x ; theta_pi)