# Bayesian Linear Regression

ridge regression as Bayesian inference
- if noise in data is large -> regularizer on weight should be large
- rigde regression returns a single model
- making predictions
  - real y will be int p(y | w, x) p(w | data) dw
  - note that it is still a gaussian -> estimation f is still gaussian of N(mu x, x Sigma x)
  - real y will be N(mu x, x Sigma x + Sigma_noise), sigma noise is called aleatoric(irreducible)
  - while ridge prediction only estimates N(mu x, Sigma_noise), the randomness from the weight estimation is gone / epistemic uncertainty(uncertainty due to lack of data)
  - ridge regression can be viewed as the full posterior(delta distribution)
- hyperparameters from cross validation
- recursive updates
  - today\`s posterior is tomorrow\`s prior