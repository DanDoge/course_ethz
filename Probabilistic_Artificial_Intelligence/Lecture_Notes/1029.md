### MCMC

gradient of ELBO
- partial_lambda E_q(lambda) log p(y | theta) - KL(q || p)
  - trouble: lambda in E_q, so the gradient cannot be pushed into the expectation
  - reparameterization trick: replace q with a random variable eps from a non-parametric distribution, consider theta = g(eps, lambda) to recover q
    - then q(theta | lambda) = phi(eps) |partial_eps g(eps, lambda)|^-1
    - and E_theta f(theta) = E_eps f(g(eps, lambda))
    - and stochastic gratidents can pass through part_lambda E_q f = E_eps part_lambda f(g)
  - part E_eps log p(y | Ceps + mu)
  = part E_eps n E_y_i log p(y_i | Ceps + mu)
  = n E_eps E_y_i log p(y_i | Ceps + mu) // draw a batch of eps, and n data points
  = n 1/m sum of part log p(y_i | Ceps_i + mu)

BBVI
- maximizing ELBO using SGD
- unbiased gradients from reparameterization / score gradients

VI summary
- reduces inference(integration) to optimization
- quality of approximation hard to analyze

### sampling

p(y | x, X) = int p(y | x, theta) p(theta | X) d theta
= E_theta p(y | x, theta)
~ 1 / m p(y | x, theta_i), with theta_i drawn from posterior
- how many samples? suppose a bounded f in [0, C], p(MC - GT > eps) <= 2exp(-2Neps^2 / C^2)
  - error decreases exponentially in N
- MCMC
  - MC, prior p(x_1), transition prob. p(x_i+1 | x_i), independent of i
  - ergodic MC: exists a finite t, s.t. each state can be reached from each state in t steps
    - 1 -> 2 -> 1: not ergodic, 1 -> 2 odd steps, 1 -> 1 even steps -> such t does not exist for all states to reach all states in **exactly** t steps
    - stationary ergodic MC has a unique positive stationary distribution s.t. lim p(x_n = x) = pi(x)
      - pi(x) independent of p(x_1)
  - detailed balance equation
    - q(x) p(x' | x) = q(x') p(x |x'), where q can be not normalized
    - then MC has a stationary distribution q(x) / Z
- metropoils hastings
  - proposal distribution r(x' | x), stronly influences performance
  - accept x' with prob min{1, q(x')r(x | x') / q(x)r(x' | x)}
    - or remain next sample as x
- gibbs sampling
  - for random vectors, at each step, pick a var i uniformly, sample from p(x_i | x_-i)
  - detailed balance also respected, no rejection needed
  - p(x_i | x_-i) = q(x_i, x_-i) / Z, Z is one dimentional integral w.r.t. x_i
- ergodic theorem
  - x_i from ergodic MC, lim 1/N sum of f(x_i) = sum of pi(x) f(x)
    - law of large number even if samples are correlated
  - in practice, better drop first t_0 samples before the chain burn-in
- MCMC for continuous RV p(x) = exp(-f(x)) / Z
  - f: energy function, convec f -> log-concave p
  - MH acc prob: min(1, R / R * exp(f - f))
  - improved proposals: move the mean toward to grad direction -> prefer lower cost function & explore outside local minima
    - guaranteed polynomial time convergence