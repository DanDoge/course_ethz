### variational inference

general bayesian learning
- posterior: int prior likelihood
- predictions : int likelihood posterior
- in general not closed form!

recall logistic regression
- p(y | x, w) = Bernoulli(y; sigmoid(wt x))
- prior w = N(0, sigma_p^2 I)
- likelihood p(y; x, w) = prod p(y_i ; x_i, w) = prod Bernoulli(y_i; sigmoid(wt x_I))
  - suppose y in {1, -1} -> prod Bernoulli(y_i; sigmoid(wt x_I)) = prod sigmoid(y_i wt x_i)
- posterior p(w ; x, y) = p(w) p(y; x, w) / Z
  - the Z is intractable, while p * p is doable

approximate inference
-  assume the joint distribution p(w, y, x) can be evaluated, but the normalizer Z can not
- variational inference: p(theta | y) = p(theta, y) / Z ~ q(theta | lambda)
  - or MCMC methods
- laplace approximation
  - second order taylor expansion around the posterior mode
  - taylor expand log p(theta | y) = something + quadratic -> Gaussian!
  - q(theta) = N with mu = argmax p(theta | y), sigma^-1 = -dd log p(mu | y) (dtheta)^2
  - for bayesian logistic regression
    - w = argmax p(w | y) = argmax p(w) p(y | w)  
      = argmax log p(w) + log p(y | w)
      = argmax - |w|/2 + sum log sigmoid(y_i x_it w), concave, SGD
    - analytically: gradient = 2 - y x p(-y | w, x)
    - cov = Xt diag(pi_i(1 - pi_i)) X
  - prediction
    - p(y | x) ~ int p(y | w) q(w | x) dw
      - note that f = wt x, reduce dimentionality from d to 1
        - s.t. can be easily approx.ed
        - for Gaussian link functions, above integral can be calculated analitically
  - problem: overconfident approx. towards to mode (e.g. multimode / skewed dist.)
- variational inference: find q = argmin KL(q || p) = int q log q / p
  - assume p, q have the same support
  - zero if p =a.e.= q
  - e.g. N(mu1, I), N(mu2, I) -> 1/2 (mu1 - mu2)^2
    - N(mu, simga^2), N(0, I) -> 1/2 sum of (sigma_i^2 + mu_i^2 - 1 - ln simga_i^2)
  - KL(q || p) favors large p on support of q, reverse/exclusive KL
    - may still reach overconfident/underestimatioon of variances
    - KL(p || q) matchs moments(in diagonal directions), forward/inclusive KL
  - min KL
    - q* = argmin E_q [log q - log p + log Z], where p can be not normalized, p is the posterior
    = argmax E_q (log p(theta, y)) + H(q)
    = argmax E_q (log p(y |theta)) - KL(q || p), where p is the prior
      - w/o the last KL, we will get MLE
  - maximizing ELBO
    - log p(y) >= E_q (log p(y | theta)) - KL(q || p)