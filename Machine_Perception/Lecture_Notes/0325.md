### deep generative models

VAE
- autoencoders not generate realistic images    
    - embedding space not organized/structured
- somehow mapping data to N(0, I)
- KL(p || q) = int log p log p/q
- p_theta(x) = int p_theta(x | z) p_theta(z) dz
- encoder model q_phi(z | x)
- data llh: log p_theta(x)
= E_z~q log p_theta(x)
= E_z~q log p(x | z)p(z) / p(z | x)
= E_z~q log p(x | z)p(z) / p(z | x) * q(z |x) / q(z | x)
= E_z~q log p(x | z) - E_z log q(z | x) / p(z) + E_z log q(z | x) / p(z | x)
= E_z~q log p(x | z) - KL(q(z | x) || p(z)) + KL(q(z | x) || p(z | x))
\>= E_z~q log p(x | z) - KL(q(z | x) || p(z))
    - first make sure output similar to input, second makes output like input
    - w/o KL loss the latent space looks less dense
- semi supervised
    - x ~ p(z, y)
- although KL loss enforces a diagonal mat, to learn a factorized representation
    - put more weight on it
    - beta-VAE: p(x | z) ~ p(x | v, w)
        - max E_x E_z log p_theta(x|z), s.t. D_KL(q || p) < delta