### training neural networks

SGD updates, chain rule
- delta_i^l: gradient w.r.t. i-th input to l-th layer
    - delta_i^(l-1) = sum part C / part z_j^l * part z_j^l / part z_i^(l-1)
    = sum delta_j^l * part z_j^l / part z_i^(l-1)
    - delta^(l-1) = sum delta_j^l * part z_j^l / part z^(l-1)
- for parameter update
    - part C / part z_j^l * part z_j^l / part theta_j^l
    - part C / part theta^l = sum part C / part z_j^l * part Z_j^l / part theta^l
    = part C / part z^l * part z^l / part theta^l
    - note by doing this, we are not abusing notations, and no more dealing with tensors
- universal approximation theorem
    - exists g(x) as NN g(x) ~ f(x), f continuous
    - g has one hidden layers, one activation
    - may not find them
    - MLP are universal Boolean/classification/function approximators