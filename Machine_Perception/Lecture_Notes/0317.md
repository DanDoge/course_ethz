### RNN

dynamical system
- s^t = f(s^(t-1); theta)
- with inputs h^t = f(h^{t-1}, x^t; theta), f canbe tanh or sigmoid
    - learn a fixed model for all time steps
    - y^t = W_hy h^t
    . flexible: one to one, one to many, many to one, many to many
- back propagation through time
    - h^t = f(h^{t-1}, x^t; theta), y^hat^t = W_hy h^t
    - L^t = |y_hat^t - y^t|^2
    - part L / part W = sum_t part L^t / part W
    - exploding and vanishing gradients
        - if h^t = W h^(t-1) -> h^t = Qt A^t Q h^1
    - l^t = l(f(h^t)) = W_hh f(h^{t-1}) + W_xh x^t
    - part l^t / part W = part l^t / part y^t part y^t / part h^t sum_k part h^t / part h^k part^+ h^k / part W
        - note h^t = h(W, h^(t-1))
    - part h^t / part h^k = prod part h^i / part h^(i-1) = prod Whh^t diag(f'(h^(i-1)))
        - |diag(f'(h^(i-1)))| < gamma, lambda_1 being the largest singular value of W_hh
        - lambda_1 < 1/gamma -> gradient vanishes
        - invert, we get gradient exploding
- solution of gradient exploding/vanishing
    - LSTM: input, forget, output gate
        - c^t = f^t * c^(t-1) + i^t * g^t # the + here gives a gradient highway -> no vanishing
        - h^t = o^t * tanh(c^t)
    . multilayer 
        - input being h_t^(l-1) and h_(t-1)^l
    - refer to slides for details