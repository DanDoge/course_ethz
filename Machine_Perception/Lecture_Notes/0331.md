### autoregressive models

pixelRNN
- model dependency by LSTM
- slow generation from dependencies
- pixelCNN: parallel
    - model dependency by CNN
    - autoregressive over color channels
    - stack masks? blind spot?
    - horizonal mask and vertical mask
    - still slow

TCN: temporal conv. net.
- wavenet: pixelcnn to audio data
    - larger dimensionality
    - dilated conv.: larger receptive field
    - increase dialation in hidden layers
    - quantile signals

VRNN
- incorporating latent var into hidden state of RNN
    - RNN gets less diverse at later time steps
- h_t = tao(x_t, z_t, h_t-1), x_t = g(h_t-1, z_t)
    - robust to noise
    - variant, but lost control
- C-VRNN: two latent spaces, continuous and descrete one
    - style and content
    - at inference time, draw from p, not q, as in VAE

autoregressive
- tractable p(x), easy to train, easy to sample
- no natural latent variable representation, but somehow doable

attn
- key, value, query
- attn weights alpha = softmax(QK^T / sqrt(D)), then weight the values
    - sqrt(D) to norm the variance: Q, K ~ N(0, 1) -> QK^T: N(0, D)
    - for auto regressive story, add a M(upper triangle with large negative) to cancel out future effects
- seq2seq