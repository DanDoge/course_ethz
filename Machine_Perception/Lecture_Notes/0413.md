### gan

good llh can still give poor samples

training
- V(G, D) = log(D(x)) + log(1 - D(x^hat))
    - prob of x from GT data to be 1 and prob of x^hat from G to be 0
    - argmax_D and argmin_G
- theoretical: p_model = p_data if D* can be reached and G and D are capable enough
    - in practice, k steps of optimzing D, avoid overfitting on finite datasets
    - keep D near optimum and G changes slowly