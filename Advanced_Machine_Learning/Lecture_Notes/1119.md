### bagging

random feature selection 
- for least square regression E argmin_beta (TXS S beta - y)^2
    - gamam = n / p
    - the asymptotic limit of risk of RFS is the same as ridge regression
    - assume n > p and Xt X is I: beta_ensemble = alpha beta_LS
    - refer to slides
- GD as boosting: fit a tree to model the gradient and add the tree to our model

boosting
- works both for reduce bias, but also reduce variance
- AdaBoost
    - train (on some weight), eval, aggregate, reweight
    - train a model that min sum w_i 1[b(x_i != y_i)]
    - get error = sum w_i 1[b(x_i != y_i)], note error in [0, 1]
    - alpha_t = log(1 / error - 1), in [0, infty]
        - b_t = b_t-1 + alpha_t b_t
    - w_i = exp(alpha_t) w_i if wrongly classified, othervise the same
        - normalize to one
    - Adaboost = ERM + exponential loss
        - min 1/n sum of L(y_i, b(x_i)), L is exp loss, b = sum alpha_t b(t)
    - Boosting trains a maximum margin classifiers
        - margin(x_i) = |sum_b_t(x)=-1 alpha_t - sum_b_t(x)=1 alpha_t|
        - so the model continues training (to maximize margin) even if training error is zero
        - with prob >= 1 - delta, generalization error is O(1/n   1/mu^2  log |H|) + the fraction of examples with margin less than mu
        - and mu decreases exponentially with number of model M
    - Adaboost/RF are spiky self-averaging interpolators
- double descent
    - in underparameterised regime: r = sigma^2 gamma / (1 - gamma), where gamma is p / n
    - in overparameterised regime: r = 1 - 1 / gamma + sigma^2 / (gamma - 1)