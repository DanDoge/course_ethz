### deep learning

problems w/ BP
- size / optimize generalization capability
- samples per weight?
    - (in CNN) what is a sample? an image or a patch or a pixel
- KL loss / momentum
- weight decay: loss + gamm sum of w^2 / (1 + w^2)
- drop out 
- optimal brain damage/surgeon
    - assume after training, we have arrived at local minima, independent weights and quadratic error function, s.t. L = sum h_ii delta_i^2
    - and delete parameter(set to zero) with low effect on loss
- autoencoder for supervised learning
    - train an encoder and decoder
    - finetune encoder and classifier
    - for linear hidden units with squared recons error -> learns PCA (up to a linear transform)
- robbins monro
    - compute theta* = argmin E_x [L(y, f(x; theta))]
    partial_theta E_x [L(y, f(x; theta))] = E_x [partial_theta L(y, f(x; theta))] = 0
    - suppose gradient is bounded, L(theta) (theta - theta*) > 0, observation of L is noised  with gamma, z_i = l(theta_i) + gamma_i, E gamma = 0, E gamma^2 = sigma^2, 
    - if eta(t) -> 0, sum eta(t) = infty, sum eta(t)^2 < infty, then Pr(theta(t) = theta*) = 1
    - E (theta^(t+1) - theta*)^2 = E (theta^t - eta(t)(l(theta^t) + gamma(t)) - theta*)^2
    = E (theta^t - theta*)^2 - 2eta(t) L(theta) (theta - theta*) + eta^2(t)(E l^2 + gamma^2)
    <= E (theta^t - theta*)^2 - 2eta(t) L(theta) (theta - theta*) + eta^2(t)(b^2 + sigma^2)
    <= E (theta^0 - theta*)^2 - 2 sum eta(i) L(theta) (theta - theta*) + const sum eta^2(i)
    - note the last term is a constant, the second will go to infty if inf L(theta) (theta - theta*) > eps