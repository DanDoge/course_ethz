### reinforcement learning

act in unknown environments
- learn a mapping from (sequence of) actions to rewards
- episodic setting: agent reset to initial state after an episode ends
    - then produce a policy
- online setting: only a single trajectory yield
- on-policy
    - agent has full control
    - while off-policy the agent has no control, but only observe data (by others)
- model-based RL
    - learn the MDP with transition prob and reward function
        - MLE estimation of transitions, note the Marcov assumption
    - trading eps_t greedy
        - with prob eps_t: pick random action
        - with prob 1 - eps_t pick best action
        - for RM conditions, converges with prob 1.
        - can not quickly eliminate clearly suboptimal actions
    - R_max
        - optimism in the face of uncertainty
        - if r(x, a) is unknown, set to R_max
        - if p(x' | x, a) is unknown, set p(x* | x, a) to 1, where x* is a fairy tale state
        - rule out suboptimal options quickly
    - hoeffding bound
        - Z_1...n in [0, C], wihth mean mu, P(mu - sample mean) <= 2 exp(-2n eps^2/C^2)
        - every T timesteps, R_max either obtains near-optimal reward or visits at elat one unknown state-action pair
            - T depends on mixing time fo MC of the MDP
    - memory requirement
- model-free RL
    - learn value function directly
    - value estimation: V(x) = E_x' R (R + gamma V(x')) ~ r + gamma V(x')
        - init v, and update v(x) = r + gamma V(x') after observing (x, a, r, x')
            - variance high!
            - v(x) = alpha_t * (r + gamma V(x')) + (1 - alpha_t) v(x)
                - temporal difference learning
                - if alpha_t satisfy RM condition and all state-action pair are chosen infinitely often, converges with prob 1.
    - q-learning
        - some initial estimate
        - q(x, a) = alpha_t(r + gamma max q(x', a')) + (1 - alpha_t) q(x, a)
        - convergence under same condition as TDL
        - optimistic q-learning: similar to R_max