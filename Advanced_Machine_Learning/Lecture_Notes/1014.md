### density estimation

why ML estimator
- while other estimators are efective and non^t biased
- consistent: theta_ML -> Theta (a delta function) instead of a distribution
- asymptotical normal: 1 / sqrt(n) (theta - Theta) converges to a normal with variance J-1 I J-1
- asymptotically effiecient: min. var, i.e. E(theta - Theta)^2
  - rao cramer bound
    - score function: lambda =  partial log P (y | theta) / partial theta
    - note that expection of score is zero (exchange partial and integral)
    - note that E (lambda * theta) = part / part theta E_y|theta (theta) = part / part theta E_y|theta (theta - Theta) + 1
    - consider cross correlation of lambda and theta, apply Cauthy-Schwartz, <= E (lambda)^2 * (E(theta - Theta)^2 - bias^2) (note that this is a general version, consistancy is not assumed)
    - E(theta - Theta)^2 >= bias^2 + something / fisher information
      - so having a (well behaved) bias may perform better in terms of deviation from the GT value
- fisher information
  - E_y|theta lambda^2
  - I^n(theta) = nI^1, assume iid
- note the Stein estimator
  - theta_JS = (1 - something) y (and thus biased) -> better than ML estimator, in terms of variance from the GT value