### regression

SVD of ridge regression
- smaller singular values shrinks fast: builtin model selection
- while LASSO penalizes sum of l1 distance
  - sparsity? suppose we have |beta|_1 = 1, then for all gt beta ||b1| - |b2|| less than 1, the best beta is not sparse, otherwise, sparse
    - and the area of "otherwise" cases grows fast for large dim.s
    - why favor sparsity? fewer computations, interpretability
- generalized ridge regression
  - with L_q regularization
  - note q < 1 breaks convexity, but statistically results similar to q = 1
- basis expansion f = sum of beta h(x)
  - cubic splines -> in ESL