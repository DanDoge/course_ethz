### probably approximately learning

PAC
- concept class: things to be learned
- class of distributions: distribution of elements
- hypothesis class
- generalization error, R(^c) = p(^c(x) != c(x)), not computable, c is unknown
    - empirical error ^R(^c) = 1/n sum 1{^c(x_i) != c(x_i)}, an unbiased estimation of generalization error
- notions: instance space X, concept(subset of X), concept/hypothesis class(set of concepts)
    - no prior knowledge/distribution of X
- PAC model
    - learn c in C if given input a suff large sample, it outputs h in H that generalizes well w.h.p.
    - for any distribution D on X
    - for any eps, delta in [0, .5]
    - if A receives a samlple Z of size poly(1/eps, 1/delta, size(C)), then A outputs ^c with p(R(^c) <= eps) >= 1 - delta
    - rectangle example
        - size of (parameters to describe) c? 4
- universal concept class is not PAC learnable: the concept class of all possible subset of finite binary sequences is not PAC learnable
- assume H = C is a finite concept class, let A returns a consistent hypothesis(R_n(c) = 0), if n > 1/eps (log |H| + log 1/delta), then ^c generalizes well
    - p(r(c) > eps) = p(r(c) > eps & r_n(c) = 0)
    <= p(exists h in H that h does not generalizes well)
    <= sum_h p(r(h) > eps & r_n(h) = 0)
    = sum_h p(r_n(h) = 0 | r(h) > eps) p(r(h) > eps)
    <= sum_h p(r_n(h) = 0 | r(h) > eps) (given the fact h makes some mistakes, it still achieves zero error on training set)
    = sum_h (1 - eps)^n
- general stochastic setting, model a distribution on X * {0, 1}
    - then gen. error is p_x,y(c(x) != y)
    - aim to attain the best solution in the hypothesis class: r(c) - inf r(c) <= eps