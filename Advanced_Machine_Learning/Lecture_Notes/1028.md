### linear classification

classification
- given training set {(x_i, y_i)} with x from a sample space, with label y in {0, 1} / {-1, +1}
  - propose c: X -> Y and {phi}, "idk"
- methods
  - prob. generative: data -> p(x, y) -> p(y | x) -> c: X -> Y
    - highly biased
    - guess a param. family of dist., MLE to compute parameters
    - assume p(x | y = y_i) have the same Var
      - then p(y | x) is linear in x (through a link function, i.e. the sigmoid function)
  - prob. discriminative: data -> p(y | x) -> c
  - discriminative: data -> c
- bayesian decision theory
  - define loss y * y -> R (y, c(x)) -> cost
  - min E l(y, c(x)), approx. by emperical loss min.
    - with n goes to infty, law of large number guarantee a convergence in prob.
    - for prob. gen. cases, we min. sum p(y | x) l(y, c(x)), not p(x, y)