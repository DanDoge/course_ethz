how to find a model
- what is one?
- given a set of experimetns that generates data x_e
  - goal: estimate hypothesis theta from hypothesis class Theta
  - chanllange: which algorithm A: X -> Theta should be learned
    - make sure the algorithm should make use of the data
  - constraint of richness: A should potentially use all of the hypothesis
    - forall theta in Theta, exists experiment e in E s.t. A(X_e) = theta
    - all hypothesis are possible outcome from some experiment
- modeling philosophy
  - replace the deterministic relation ship with a prob. distribution: P(theta | X_e)
  - Gibbs distrbution: P(theta | X) ~ exp(-R(theta, X) / temprature) = exp(-(R(theta, X) - F(X)) / temp), where F(X) is the free energy
  - selecting a model means choosing a P or a R
  - description length: -log P(theta | X) = (R(theta, X) - F(X)) / tao
  - measure generalization: E_x1x2 E_theta|x1 (-log P(theta | x_2)), description length on test data
  - when a hypothesis is poor -> high cost -> small prob. -> large description length
    - how would E_x1x2 E_theta|x1 (-log P(theta | x_2)) behave, p(theta | x1 small) but the des. len. large: this principle can be unstable
  - apply Jensen: E_x1x2 E_theta|x1 (-log P(theta | x_2)) >= E_x1x2 (-log E_theta|x1 P(theta | x_2)) = E_x1x2 (-log R(x1, x2)), where R is a kernel function(symmetric) in [0, 1]
    - when R = 1, only one hypothesis left, and it works for all x
    - when R = 0, no overlap between the support
    - goal: max the kernel value(which means the max prob. algorithm should be stable w.r.t. choice of dataset)
- another goal: the posterial should use all hypothesis class when average over all experiments
  - Pi(theta | X) = E_e P(theta | x_e) ~ |Theta|^-1
    - e.g. relabeling/permutation of the graph will be different experiment of the same data? unsure -> seems to be the case
    - but does that means the p(theta | X) shoule be board and picky at the same time? trade off
  - min. KL devergence: E_x(Pi(theta | X) || |Theta|^-1) = log |Theta| - max. H(Theta | X) (conditional entropy)
    - why KL? for mathematical simplicity
- model score: min_A (generalization error + KL divergence) >= -max_A E_x1x2 log(|Theta| R(x1, x2))
  - number of bits in the hyopthesis class, reduced by (1 - R), some of the hypothesis are ruled out by the data