prob. discr. classifiers
- logistic regression: p(y | x) = sigmoid(w x)
  - MLE: analytically intratable, but differentiable
    - do GD on NL(w), w^(k+1) = w - eta(k) d NL(w^k)
    - eta(k) should min NL(w^(k+1)) -> approax NL near the beighborhood of w^k -> taylor expansion
      - optimal lr
      - zigzag behaviour explained if using optimal lr
        - NL(w^(k+1)) = NL(f(eta^k)), take derivative ->  d NL(f(eta^k)) * d f(eta^k) = 0
          - where the first one is d NL(w^(k+1)), the second is d NL(w^k)
          - successive gradient steps are orthogonal
      - add momentum w^(k + 1) = w^k - eta d NL + mu (w^k - w^(k - 1)), with mu in [0, 1]
    - Newton's method w = argmin NL(w) with w in N(w, eps)
      - not limited to gradient direction
      - no learning rate, better updates, require access to Hessian
        - or only a diagonal of Hessian
      - ILSR for regression: newton's methods in logistic regression
  - with n -> infty, MLE of weight is unbiased
    - what if d -> infty
      - MLE are biased in magnitute and tends to be overconfident
        - w_MLE -> alpha w, and variance also converges to sigma*
- laplace method
  - p ~ exp(-r(w)), taylor expand r(w)
- BIC: assume flat prior, log p(x, y) = const + log p(x, y | w) - d/2 log N