### review rao cramer bound

note that only E (theta - Theta)^2 is of interest, while bias is a secondary property

Bayesian methods requires integration(out the details)
- while frequentist methods only differentaition

suppose the true model is not in our model class
- sqrt(n)(theta - theta_0) -> N(0, J-1 I J-1)
  - where J is -Hessian, theta_0 is the best reachable model
  - if theta_0 is Theta -> J = I

### regression

argmin_f E(y - f(x))^2
- let f = f* + d, where f* is E(y | x)
- substitute, and min. is achieved when d(x) == 0
- y = beta_0 + sum of x_i beta_i, beta_0: bias/intercept
- assume additive Gaussian noise in observation
  - beta ~ N(beta, (XtX)-1 sigma^2)
- optimality
  - LSE of beta has the smallest variance among all linear unbiased estimates
    - for a new data point, LSE estimates x_(n+1) beta
      - note this estimator is unbiased
      - for all other not biased estimator cy, E(cy) = x_(n+1) beta (from unbiasedness), let cy = x_(n+1) beta + x D y -> x D X = 0
      - smallest var.
  - note the bias-variance trade-off: LSE is the best(in terms of min. MSE) unbiased estimators
    - E_f|D E_y|x (f(x) - y)^2: adding/subtracting E(y|x), and E_D f(x)