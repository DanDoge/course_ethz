### ensembles

bagging
- classification trees
    - each node ask a question
    - problem: shallow trees has high bias, and low var; deep trees has low bias but high var(the model tries to model the random noise in sampling)
    - bagging: reduces bias
- bootstrap set: draw m bootstrap sets(with repetition, and the size is the same as original set), train m base models, aggregate by averaging / voting
    - E of loss bag model <= E of each model, the difference determined by variance of models
    - proof sketch: E (y - bag(x))^2 = noise + bias^2 bag(x) + var b(x)
    = E (y - E bag(x))^2 + var b(x)
    = E (y - E b(x))^2 + var b(x)
    = noise + bias^2 b(x) + 1 / n var b(x)
    - non-diverse models leads to bad results
- break dependence of models
    - independent model -> independent data, but bootstrap set are not independent
    - random forests: choose m faetures at random and splitting with one of features(at one node)
        - reduce the correlation between trained base trees(suppose one feature is strong predictor, then most of trees will split on it)
    - random feature selection induces implicit regularization
        - y = x beta + eps, ridge estimator is asymptotically optimal
        - T selects examples, S selects features