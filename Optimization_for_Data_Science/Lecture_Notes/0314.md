smooth and strongly convex
- squared distance of x to x*
    - decrease by a constant factor, up to some noise
    - plug in vanilla analysis and sufficient decrease
- f(x_t) also bounded: Nabla f(x_t) (x_t - x*) >= 0
- O(log(1 / eps)) called linear convergence

projected gradient descent
- constrained optimization: min f(x) s.t. x in X
- PGD: x_t+1 = Pi_X(x_t - gamma Nabla f(x_t))
- properties of projection
    - (x - Pi_X(y))^t(y - Pi_X(y)) <= 0
    - dist(x, Pi_X(y)) + dist(y, Pi_X(y)) <= dist(x, y)