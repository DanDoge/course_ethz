### introduction

explain data from an unknown distribution
- quality: explanation ability
- MST: for stepwise algorithm, max_t joint information of possible ST at time t
    - return random ST from A_t(X`) and A_t(X``)
    - reverse delete better than kruskal, than prim
- expected risk, E_X l(H, X)
- halfplane: l(H, X) = I_{I_{x in H} not= y} = I_{x in H symm. diff. H*}
- weak law of large numbers, |l_n(H) - l(H)| <= eps
    - not uniformly converge for all H, l(H) <= l_n(H) + eps does not hold for all n
    - true sufficient condition: sup |l_n(H) - l(H)| <= eps w.h.p.
        - hold in some cases
- map of learning
    - algorithm: training data -> hypothesis
    - validation: empirical risk from test data
    - overfitting: theory(H and l) complex
    - underfitting: theory too simple
    - generalization does not mean explanation is good: blind explanation w.r.t. training data
    - regularization: trade high bias for low variance
    - early stopping: as training t grows, bias(deviation from theory) drops, variance(sensitivity to training data) increases
- theory: informative, robust, efficient ERM
- avg complexity: lack of knowledge / only doable for specific distribution
    - while there is only one worst case complexity 
- estimation / optimization error tradeoff
    - small scale learning: aim for small optim. error
    - large scale learning: give up optim. precision due to time
    - optim error at most eps after c(eps) steps
        - bound c as eps to 0