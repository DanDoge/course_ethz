### modern second order methods

newton's method depends on the initialization
- under what condition does it global conveges
    - negative exanples for nonconvex and strictly convex functions
- design second order methods with global convergence
    
strongly convex, Lipschitz gradient
- neuton method with step size gamma
- global linear convergence: constant worse than GD

overcoming local nature of newton
- w/ line search, s.t. f(x_t+1) has sufficient decrease
- damped stepzize 1 / (1 + ||nabla f(x) ^(-.5) nabla f(x)||)
- regularization: (gamma I + Hessian)^-1
- trust region 