### gradient descent

bound f(x_t) - f(x*)
- f(x*) >= f(x_t) + part f(x_t)(x* - x_t)
- part f(x_t) = (x_t - x_t+1) / gamma
- plug in, apply cosine theorem
    - g_t T (x* - x_t) = gamma/2 |g_t|^2 + 1/2gamma (|x_t - x*|^2 - |x_t+1 - x*|^2)
    - sum up, sum g_t (x_t - x*) = sum gamma/2 |g_t|^2 + 1/2gamma (|x_0 - x*|^2 - |x_t+1 - x*|^2)
        - which is then a upper bound of sum f(x_t) - f(x*)
- upper bound for average error 
    - mean g_t (x_t - x*) = mean gamma/2 |g_t|^2 + 1/2gamma |x_0 - x*|^2 
    - last update is not necessarily the best one
    - can |g_t| be bounded
    - how to choose gamma
- assume all gradient of f are bounded in norm
    - suppose |x_0 - x*| <= R, and |part f| <= B, choose stepsize R/Bsqrt(T)
        - for avg error <= eps -> T > R^2B^2 / eps^2
        - dimention-independent
        - holds for avg and best iterate
        - R, B can be large / depend on d
        - slow
        - what if do not know R?
- not too curved
    - smooth if f(y) <= f(x) + df (y - x) + L/2 |x - y|^2
    - does not require convexity, always include concave functions
    - iff convexity of L/2 xtx - f(x)
    - operations preserve smoothness, same as convexity, except pointwise maximum
    - iff Lipschitz continuity of df (if f is convex)
    - with stepsize 1/L, gradient descent will make progress every step
        - plug in definition
    - f(x_t) - f(x*) <= L / 2T |x_0 - x*|^2
        - bound square gradient by sufficient decrease
        - since we make progress every step, this also bounds the last iteration
- accelarated GD
    - Nesterov: y_t+1 = x_t - 1/L df(x_t)
    z_t+1 = z_t - (t+1)/2L df(x_t)
    x_t+1 = (t+1)/(t+3) y_t+1 + 2 / (t+3) z_t+1
    - f(y_T) - f(x*) <= 2L |z_0 - x*|^2 / T(T+1)
- strongly convex functions
    - f(y) >= f(x) + df(x)(y - x) + mu/2 |x - y|^2
    - iff convexity of f(x) - mu/2 xtx
    - for smooth an strongly convex functions O(log(1/eps)) steps