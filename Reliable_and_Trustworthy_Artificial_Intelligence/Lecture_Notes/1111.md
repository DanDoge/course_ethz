### certified defenses

training the network to be experimentally robust / provably robust
- find theta to min rho(theta)
    - E max L(theta, z, y), z in gamma(NN#(S(x)))
    - z is from the symbolic region pushed through the NN
- backpropagation through abstract transformers
    - not concrete transformers
    - input x -> abs shape S(x) -> push through NN -> final zonotope -> concrete to z_1...n -> compute L
    - infty number of z's, how to eliminate gamma?
        - define loss = max(z_i - z_y), note that this is affine transform, done!
        - what about CE loss? z in [z_l, z_h], pick l_i if y = i, u_i otherwise, to max. loss; apply softmax, then CE
            - tricks: state with small region and gradually grow during training, dynamically weighing-in statndard CE loss and robustness CE loss
    - observations: cheap relaxations scales to large networks, but a lot of  garbage points included -> although the network capacity increases, but the more points are we training/assigning label to -> acc. drops substantially
        - more precise relaxations may lead to worse results than box
    - questions
        - is there a network with perfect acc. s.t. analyzing it with Box is exact -> Yes, it does exist, but with impractical construction
        - why better relaxations producing worse results
            - hypothesis: more complex abstractions leads to more difficult optimization problems
            - COLT: bridging the gap
                - find a x_i in layer i that the final loss at final layer is maximized: PGD style training but project to zonotope in some intermediate layer
                - max x in Z L(x, y) = max e in [0, 1]^k L (Ae, y)
                    - projection in Zonotope via projection onto box