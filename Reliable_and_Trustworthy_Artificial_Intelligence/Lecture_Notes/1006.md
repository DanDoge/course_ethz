### mathematical certification of NN

attack and defenses work well in practice but with no formal guarantees
- goal: automated verifier to prove properties of realistic networks
  - certifying large systems using NN
  - proving robustness of NN
  - learning interpretable sprcs of NN
  - comparing NN

problem statement
- givne a neural network N, a property over inputs phi(pre-condition), a property over outputs psi(post-condition)
- forall i in I, if i has phi -> N(i) has psi holds or return a violation
- instantiating to certifying robustness
  - define phi: examples a infty ball around a point x, or some other regions(brightening attack)
  - define psi: e.g. all points classifies to some fixed class c, for all class i, prob_c >= prob_i, where the prob is the output of the last layer/the second last layer
  - how to prove: phi captures unbounded number of points, cannot enumerate

  certification methods
  - sound: if the program violates a property, the methods terminates and states the property is violated, typically refered to as certification methods
  - unsound: if hte program violates a property, the methods can potentially terminate stating the property is satisfied, e.g. advsersarial attack methods are unsound: advasarial examples may be missed while they actually exist(property to be certified: the network is defended)
  - complete: if it is able ot prove the property if it actually holds
  - incompelete:: if it cannot guarantee that it can prove a property that is actually holds
  - for certification we want: sound, scalable, and precise(as complete as possible) algorithm, scalability and precision may be a tradeoff
  - not possible to constract a sound and complete varifier
    - for some restricted computations(e.g. NN), possible to construct a sound and complete(may not be scalable) certifiers

cover two sound methods
- incomplete but scalable: Box, Zonotope, DeepPoly
  - bound propagation through the neural network
  - push phi through the network, computing a over-approximation of the effect of each layer(some points inside phi_n cannot be produced by any concrete point inside phi), check all points inside phi_n satisties psi, even for the infisible points
  - simple property can tolarate more over approximation
- complete but not scalable: MILP, LP

how to produce convex shapes
- what is the convex approximation
- how are these approximations produced(abstract transformer)

Box/ intervals
- abstract transformers for ReLU neural networks
  - optimal transformer: all possibility are considered, but not complete(includes some infeasible points)
- addition: addition of the lower bound and upper bound
- negation, ReLU, multiplication: simple
- sound but not exact
  - graph legends, bias
  - may be enough to verify and desired property