### zonotope convex relaxation

incomplete but somehow scalable
- box loses too much precision. in both ReLU and affine transformation
  - consider [ReLU(x), ReLU(x)]
- while zonotope will not lose precision for affine transformation
  - a more extensive version of Box, but variables can be related
- in zonotope, abstract neurons are X = a + A epsilon, with epsilon(noise terms) in [-1, 1]^n, and (a, A) controls the magnitude of noise
  - with sharing of epsilon_k, relations of x_i, x_j can be expressed in a_ik, a_jk
  - geometrically, X is a poytope centered around a
  - while the equivalent box will be min(a + A epsilon), max(a + A epsilon)
- transform a zonotope
  - affine layers y = W x + b
    - y = W A epsilon + W a + b
  - ReLU y = max(0, x)
    - we look at one transformer in practice
      - obtain box bound for x: exploring epsilon = 1 and -1
      - u_x < 0, y = 0, l_x > 0, y = x
      - if l_x < 0 < u_x -> construct a zonotope that the y=ReLU(x) is covered
      - lambda x < y(x) < lambda x  - lambda l_x -> y(x) = lambda x - c lambda l_x
        - where c in [0, 1], let c = (epsilon_new + 1) / 2
        - substitute, done... but number of error terms increases each time a ReLU crosses zero
      - optimal ReLU transformer: triangle, best convex approx. if only one neuron is considered
        - for more than one neurons, consider (x1, x2)_1 < 2
- how to project to it for provable training