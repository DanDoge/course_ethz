### randomized smoothing for robustness

randomized smoothing
- given base classifier f, generate g from f with some statisical robustness guarantees
    - the construction of g assumes no knowledge of f, and thus scale to large network
    - g(x) = argmax_c P(f(x + eps) = c), where eps ~ N(0, sigma^2)
    - suppose for a c_A, P(f(x + eps) = c_A) >= p_A >= p_B >= max_c P(f(x + eps) = c)
        - then g(x + delta) = c_A for delta less than sigma / 2 (Phi^-1(p_A) - Phi^-1(p_B))
        - while theoritically possible to compute p_A and p_B, exact inference solves do not scale
        - increase noise level can increase certification radius, but reduce accuracy
    - certified accuracy: count number of points in the test set with radius R >= T and the c_A matches the test label
    - sampling to get  p_A and p_B
        - sample a small numner of points to get c_A
        - then sample a large numner fo samples to get a lower bound prob that p(x + eps) = c_A (Pearson, etc.)
        - p_B is upper bounded by 1 - p_A: return sigma Phi^-1(p_A) if p_A > .5
    - increasing sigma -> lower accu
        - c_A might be wrong in the first place
        - p_A might be small or similar with the next best label
        - note the results only holds with high prob.
    - increasing number of samples will only slowly grows the radius 
        - suppose the network always classifies points to c_A
    - at inference time: also sampling
        - sample points in the neighbor
        - get top two indices
        - if BinomPValue(n_A, n_A + n_B, 0.5) <= alpha
            - returns palue of the two classes are indistinguishable
            - high alpha: only make prediction when confident
            - returns wrong class with prob at most alpha
- geeneralize smoothing to image transformations (instead of just adding noise)