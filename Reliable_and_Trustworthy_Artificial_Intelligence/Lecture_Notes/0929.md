# adv attack

targeted/untargeted attack
- misclassify the input to a specific label
  - goal: f(x + eta) = t
  - TFGSM: targeted fast gradient sign method
    - compute perturbation: eta = epsilon * delta_x loss_t(x), or delta_t, where the t stands for the tranformations: rotation, translation, etc.
    - perturb the input: x` = x - eta (to min. the loss_t)
    - check if f(x`) = t
    - x` is guaranteed to stay in the box \[x+-epsilon\], no reprojection needed
    - fast but not optimal
- or any wrong label
  - untargeted FGSM
    - eta = epsilon * delta_x loss_s(x), where s is the origin label of x
    -  x` = x + eta (to max. the loss_s)
    - and check if the label changes

white box: attacker knows the model, param, architecture
- or black box: params not known
- adv examples are transferrable, thus train an own mirror network of the bb network and attack the mirror network often also works
  - only look at white box attacks later on

norm
- l_p norm
- l_2 norm can be small if there are many small changes to lots of pixels
- l_infty norm: maximum noise added to any coordinate

targeted attack with small changes
- change prediction, with small eta
- optimization problem
  - min p norm of eta, s.t. f(x + eta) = t, and x + eta in [0, 1] (is still an image)
  - define an obj s.t. if obj_t(x + eta) < 0, then f(x + eta) = t, note not iff
    - obj_t = loss_t - 1
    - obj_t = max(0, 0.5 - p_t(x + eta))
  - min p norm of eta + c * obj_t(x + eta)
  - how to min infty norm of eta, hard cosntraint? x + eta in [0, 1] still hard constraint?
    - PGD: projected gradient descent
      - fit all coord within the box \[-x, 1 - x\]
      - PGD attack: always more than one step unless we find a counter example on the first round?
        - not necessarily always need to reproject
        - not always have to proejct to boxes
        - actural decision boundary not known
        - step size typically smaller than epsilon
        - oepn problem: project to nto a box, but an arbitrary convex shape?
          - convexity v.s. cost of projections
    - LBFGS-B optimizer?
      - pass constraints to the optimizer, internally does the projection
      - eta infty norm can also be feed into the optimizer

attacks as of today
- FGSM(produced examples will on the boundary of the region), PGD, C&W(try to make small changes)

adv defences
- adv accu: the ratio of data point that are classified correctly and the network is robust around the point
  - canculated adv accu is an UPPER BOUND of true adv accu, because not always will we find an adv example even if it exists
- test/natural accu: the ratio of data point that are classified correctly
- adv accu and test accu can be a tradeoff
- defense as an optimization problem
  - find theta, min p(theta), p is for each point, the expetation value of max loss in the neighboorhood of the data point
  - find a mini batch
  - PGD attack to find the max loss x`, optimize the outer min. problem
    - note x` does not necessarily have to be an adv exmaple
- model capacity matters: larger network are more defendable
  - small network will suffer from low test accu if trained adv.ly
- PGD defenced network tends to be better than FGSM networks
- images, NLP, code, audio, finaicial and other fields