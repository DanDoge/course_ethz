### logic and deep learning

query the network beyond adv exanples
- adv examples are just a special case of query
    - find image s.t. class is not some class, distance with in some margin
- what are queries and how to perform queries with constraints
    - define logic: no quantifiers(all, exists)
    - class(i) = 9 -> NN(I)\[9\] > all other classes
    - with i being free variable, find a i s.t. the constraints are satisfied
    - translation T that T(phi) is a differentiable loss function
        - T(phi)(x) = 0 iff x satisfies phi
        - t_1 <= t_2 -> max(0, t_1 - t_2), t_1 neq t_2 -> [t_1 = t_2]
            - all other cases reduce to these two
        - take box constraints to the optimizer: L-BFGS-B, instead of translate them
    - usage in semi-supervised learning: for unlabeled data: p < eps or p > 1 - eps

enforce properties the network should satisfy
- fine theta s.t. max. E[forall z phi(z, s, theta)], the expected value of property is maximized.
    - min E[max_z ¬ phi(z, s, theta)], min the perportion that the property is violated
    - min E[T(phi)(z_worst, s, theta)], z_worst = argmin_z T(¬phi)(z, s, theta)
- phi() = |x - z| < eps => NN(z) > delta
    - ¬phi() = |x - z| < eps and NN(z) <= delta
        - note the first one is a L-infty ball