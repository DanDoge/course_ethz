### deeppoly

more precise than Zonotope when approximating ReLU
- popupar numerical relaxations: box, octagon(x1 + x2 < l, with coef being +-1), zonotope, polyhedra(Ax < b)
- shape: for each x_i, keeping an interval constraint, two relational constraints a_i_l < x_i < a_i_u
  - with relational constraints of the type sum of w_j x_j + v
  - precise transformation for affine layers, less precise then polyhedra
    - but more scalable than the latter: constant time for ReLU
- transform
  - at the first layer, a_i_l/u from lower/upper bounds
  - for affine layers, x_i >= / <= sum of previous neurons, how to get bounds?
    - backsubstitution x_i to previous layers all the way to beginning
      - can be done in parallel
      - still not the most precise bound compare to linear solver bounds
  - for ReLU layers x_j = ReLU(x_i), if u_i <= 0, a_j_i/u = 0, l/u_j = 0, similar for strictly positive cases, which copies all bounds from x_i
    - for crossing cases, x_j >= 0, x_j <= the same upper bound as in Zonotope, upper bound = u_i, lower bound = 0

automated verifier
- given a program and a property, output valid or give a counter example
  - in general, not decidable
- abstract interpretation
  - soundness, gamma(F#(z)) contains F(gamma(z))
    - where gamma is concretization, which transforms a abstract shape to concrete points, e.g. points in L-infty box
    - F# is abstract transformer, works on abstract/symbolic shape
    - whereas F is the concrete transformer, hard to compute its transformation for all poitns in x
  - optimality
    - if for all sound transformers, F' is not more precise than F_best
    - for Box, both affine and ReLU are optimal
    - for Zonotope, affine is exact and optimal, but no single best transformer for ReLU
    - note Box and Zonotope are symbolic shapes, not to be confused with transformers on these shapes