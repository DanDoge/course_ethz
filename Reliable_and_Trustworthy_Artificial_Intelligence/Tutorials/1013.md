### Q&A

cross entropy when both of them are not one-hot
- use KL divergence
- note cross entropy is -sum of p log q
  - and kl is sum of p log p / q, note that sum of p log p is constant?
    - note the detach operation